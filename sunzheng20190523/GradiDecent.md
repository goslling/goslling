# 梯度下降算法总结

## 1.Batch Gradient Descent （BGD）

**梯度更新规则:**

BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310213628078-1772953909.png)

```
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

缺点：**由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型。**

## 2.Stochastic Gradient Descent (SGD)

**梯度更新规则:**

和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 **SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310214057443-2087742064.png)

```
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。**缺点是SGD的噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向**。**所以虽然训练速度快，但是准确度下降，并不是全局最优**。**虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。**

## 3.Mini-Batch Gradient Descent （MBGD）

**梯度更新规则：**

**MBGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310214554893-313788349.png)

```
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

**超参数设定值:  n 一般取值在 50～256**

**缺点：（两大缺点）**

1. **不过 Mini-batch gradient descent 不能保证很好的收敛性，**learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）**对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（**会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。**）
2. SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，**我们更希望对出现频率低的特征进行大一点的更新。LR会随着更新的次数逐渐变小。**

#### 鞍点

一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得算法陷入其中很难脱离出来，因为梯度在所有维度上接近于零。

长期以来，人们普遍认为，神经网络优化问题困难是因为较大的神经网络中包含很多局部极小值（local minima），使得算法容易陷入到其中某些点。**到2014年，一篇论文《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》，提出高维非凸优化问题之所以困难，是因为存在大量的鞍点而不是局部极值。**

![img](https://img-blog.csdn.net/20180204002810955?)

鞍点（saddle point)这个词来自z=x2−y2  的图形，在x轴方向向上曲，在y轴方向向下曲，像马鞍，鞍点为（0，0）。

**神经网络优化问题中的鞍点即一个维度向上倾斜且另一维度向下倾斜的点。**

鞍点：梯度等于零，在其附近Hessian矩阵有正的和负的特征值，行列式小于0，即是不定的。
鞍点和局部极值的区别：
鞍点和局部极小值相同的是，在该点处的梯度都等于零，不同在于在鞍点附近Hessian矩阵是不定的(行列式小于0)，而在局部极值附近的Hessian矩阵是正定的。

在鞍点附近，基于梯度的优化算法（几乎目前所有的实际使用的优化算法都是基于梯度的）会遇到较为严重的问题：
鞍点处的梯度为零，鞍点通常被相同误差值的平面所包围（这个平面又叫Plateaus，Plateaus是梯度接近于零的平缓区域，会降低神经网络学习速度），在高维的情形，这个鞍点附近的平坦区域范围可能非常大，这使得SGD算法很难脱离区域，即可能会长时间卡在该点附近（因为梯度在所有维度上接近于零）。

### 指数加权平均

后面的优化算法都使用了指数加权平均的思想。我查了下指数加权平均，在这记录下。

#### 1.什么是指数加权平均

**指数加权平均**(exponentially weighted averges)，也叫指数加权移动平均，是一种常用的序列数据处理方式。

它的计算公式如下：

![img](https://upload-images.jianshu.io/upload_images/1667471-2e306206d9923eda.png)

其中，

- θ_t：为第 t 天的实际观察值，
- V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值，
- β： 为 V_{t-1} 的权重，是可调节的超参。( 0 < β < 1 )

例如：

我们有这样一组气温数据，图中横轴为一年中的第几天，纵轴为气温：

![img](https://upload-images.jianshu.io/upload_images/1667471-a8c6d09f71ed9d3c.png)

直接看上面的数据图会发现噪音很多，这时，我们**可以用 指数加权平均 来提取这组数据的趋势，**按照前面的公式计算：这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t：

![img](https://upload-images.jianshu.io/upload_images/1667471-41c46f96e934843e.png)

将计算后得到的 V_t 表示出来，就得到红色线的数值：

![img](https://upload-images.jianshu.io/upload_images/1667471-7d82e7b89e860299.png)

可以看出，红色的数据比蓝色的原数据更加平滑，**少了很多噪音**，并且**刻画了原数据的趋势**。

指数加权平均，作为原数据的**估计值**，不仅可以 **1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来**。

所以应用比较广泛，在处理统计数据时，在股价等时间序列数据中，CTR 预估中，美团外卖的收入监控报警系统中的 hot-winter 异常点平滑，深度学习的优化算法中都有应用。

#### 2.为什么在优化算法中使用指数加权平均

上面提到了一些 指数加权平均 的应用，这里我们着重看一下在优化算法中的作用。以 Momentum 梯度下降法为例，**Momentum 梯度下降法**，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行**速度几乎总是快于标准的梯度下降算法**。**这是为什么呢？**

让我们来看一下这个图，

![img](https://upload-images.jianshu.io/upload_images/1667471-07d825d3e2624537.png)

例如这就是我们要优化的成本函数的形状，图中红点就代表我们要达到的最小值的位置， 假设我们**从左下角这里出发开始用梯度下降法**，那么蓝色曲线就是一步一步迭代，一步一步向最小值靠近的轨迹。可以看出**这种上下波动，减慢了梯度下降法的速度**，而且无法使用更大的学习率，因为如果用较大的学习率，可能会偏离函数的范围。

如果有一种方法，可以使得在纵轴上，学习得慢一点，减少这些摆动，但是在横轴上，学习得快一些，快速地从左向右移移向红点最小值，那么训练的速度就可以加快很多。

这个方法就是动量 Momentum 梯度下降法，它**在每次计算梯度的迭代中，对 dw 和 db 使用了指数加权平均法的思想**，

![img](https://upload-images.jianshu.io/upload_images/1667471-eedf9342a4bce813.png)

这样我们就可以得到如图红色线的轨迹：

![img](https://upload-images.jianshu.io/upload_images/1667471-f9e70b57daae0359.png)

可以看到：
**纵轴方向**，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。
**横轴方向**，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。
在抵达最小值的路上减少了摆动，加快了训练速度。

#### 3.β 如何选择？

根据前面的计算式子：

![img](https://upload-images.jianshu.io/upload_images/1667471-41c46f96e934843e.png)

将 V_{100} 展开得到：

![img](https://upload-images.jianshu.io/upload_images/1667471-2d20a8e468e40bda.png)

这里可以看出，V_t 是对每天温度的加权平均，之所以称之为指数加权，是因为加权系数是随着时间以指数形式递减的，**时间越靠近，权重越大**，越靠前，权重越小。

![img](https://upload-images.jianshu.io/upload_images/1667471-485da343fbd96353.png)

再来看下面三种情况：

当 β = 0.9 时，指数加权平均最后的结果如图**红色线**所示，代表的是最近 10 天的平均温度值；
 当 β = 0.98 时，指结果如图**绿色线**所示，代表的是最近 50 天的平均温度值；
 当 β = 0.5 时，结果如下图**黄色线**所示，代表的是最近 2 天的平均温度值；

![img](https://upload-images.jianshu.io/upload_images/1667471-7d82e7b89e860299.png)

![img](https://upload-images.jianshu.io/upload_images/1667471-6fd989467bcb6121.png)

**β 越小，噪音越多**，虽然能够很快的适应温度的变化，但是更容易出现奇异值。

**β 越大，得到的曲线越平坦**，因为多平均了几天的温度，这个曲线的波动更小。
 但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 ，
 曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。

通过上面的内容可知，β 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，**一般 0.9 的效果就很好**。

## 4.Momentum

在上面的指数加权平均中已经介绍了，下面看看Momentum其物理意义是什么

当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。
加入的这一项，**可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。**

## **5.Nesterov Accelerated Gradient**

**用 θ−γv_t−1 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310223504871-1752782669.png)

从山顶往下滚的球会盲目地选择斜坡。更好的方式应该是在遇到倾斜向上之前应该减慢速度。

Nesterov accelerated gradient（NAG，涅斯捷罗夫梯度加速）不仅增加了动量项，并且在计算参数的梯度时，在损失函数中减去了动量项，即计算∇θJ(θ−γνt−1)，这种方式预估了下一次参数所在的位置。即：

νt=γνt−1+η⋅∇θJ(θ−γνt−1)，θ=θ−νt

如下图所示：

![img](http://img.mp.itc.cn/upload/20170404/f588066c9a604e00a0c2d042c3aec15d.jpeg)

详细介绍可以参见Ilya Sutskever的PhD论文[9]。假设动量因子参数γ=0.9，首先计算当前梯度项，如上图小蓝色向量，然后加上动量项，这样便得到了大的跳跃，如上图大蓝色的向量。这便是只包含动量项的更新。而NAG首先来一个大的跳跃（动量项)，然后加上一个小的使用了动量计算的当前梯度（上图红色向量）进行修正得到上图绿色的向量。这样可以阻止过快更新来提高响应性，如在RNNs中[8]。

通过上面的两种方法，可以做到每次学习过程中能够根据损失函数的斜率做到自适应更新来加速SGD的收敛。下一步便需要对每个参数根据参数的重要性进行各自自适应更新。

## 6.Adagrad （Adaptive gradient algorithm）

这个算法就**可以对低频的参数做较大的更新**，**对高频的做较小的更新**，也因此，**对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性**，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。

 **梯度更新规则:**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225616227-189502936.png)

其中 g 为：t 时刻参数 θ_i 的梯度

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225654424-79255399.png)

如果是普通的 SGD， 那么 θ_i 在每一时刻的梯度更新公式为：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225725091-491968649.png)

但这里的 learning rate η 也随 t 和 i 而变：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225741325-483629031.png)

**其中 G_t 是个对角矩阵， (i,i) 元素就是 t 时刻参数 θ_i 的梯度平方和。**

**Adagrad 的优点是减少了学习率的手动调节**，超参数设定值：一般η选取0.01

**缺点：**它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小。

## **7.Adadelta**

这个算法是对 Adagrad 的改进，

和 Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311101514058-575958058.png)

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 ，所以可以用 RMS 简写：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311101814653-889506208.png)

其中 E 的计算公式如下，t 时刻的依赖于前一时刻的平均和当前的梯度：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102007929-961929459.png)

**此外，还将学习率 η 换成了 RMS[Δθ]，这样的话，我们甚至都不需要提前设定学习率了：**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102110181-1239048345.png)

## **7.RMSprop**

RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。

**RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的**，

**梯度更新规则:**

RMSprop 与 Adadelta 的第一种形式相同：**（使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率η）**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102340145-62793612.png)

**超参数设定值:**

**Hinton 建议设定 γ 为 0.9, 学习率 η 为 0.001。**

## 8.Adam：Adaptive Moment Estimation

这个算法是另一种计算每个参数的自适应学习率的方法。**相当于 RMSprop + Momentum**

除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的**指数衰减平均值**：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105115382-532376237.png)

如果 mt 和 vt 被初始化为 0 向量，那它们就会向 0 偏置，所以做了**偏差校正**，通过计算偏差校正后的 mt 和 vt 来抵消这些偏差：

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105330568-34539188.png)

**梯度更新规则:**

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105355059-406281512.png)

**超参数设定值:**
**建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8**

**实践表明，Adam 比其他适应性学习方法效果要好。**

------

几种梯度下降算法的效果对比：

 ![SGD optimization on saddle point](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105558593-251578131.gif)

![img](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311110108768-2113908893.gif)

上面两种情况都可以看出，Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。

由图可知自适应学习率方法即 Adagrad, Adadelta, RMSprop, Adam 在这种情景下会更合适而且收敛性更好。

## 如何选择优化算法

**如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。**

**RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。**

**Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，**

**随着梯度变的稀疏，Adam 比 RMSprop 效果会好。**

整体来讲，**Adam 是最好的选择**。

很多论文里都会用 SGD，没有 momentum 等。**SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点**。

如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。

 